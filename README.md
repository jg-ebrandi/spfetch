# ğŸš€ spfetch

![spfetch_lg](https://github.com/user-attachments/assets/c66f083b-3899-4482-94da-1f85609b357e)



<p align="center">
  <b>Simple. Streaming. MFA-ready.</b><br>
  List and fetch files from <b>SharePoint</b> via <b>Microsoft Graph</b> with clean APIs and cloud-native downloads.
</p>

---

## âœ¨ What is spfetch?

`spfetch` is a Python library to:

- ğŸ“‚ List SharePoint folders  
- â¬‡ï¸ Download files in streaming mode  
- â˜ï¸ Send files directly to S3 / GCS / ADLS  
- ğŸ” Authenticate with MFA (Device Code Flow)  
- ğŸ“Š Optionally read small files into pandas  

Built for data engineers and analytics workflows that need reliability and simplicity.

---

## ğŸ¤” Why?

Accessing SharePoint programmatically usually involves:

- ğŸ” Complex authentication flows (MFA included)  
- ğŸ” Confusing browser URLs vs API paths  
- ğŸš¦ Throttling (HTTP 429)  
- ğŸ§  Memory issues when handling large files  

`spfetch` solves this with:

- âœ… MFA-compatible authentication  
- âœ… Streaming-first downloads (no full in-memory load)  
- âœ… fsspec integration (cloud-native destinations)  
- âœ… Minimal and explicit API  

---

## ğŸ“¦ Installation

### Core

```bash
pip install spfetch
```

---

### â˜ï¸ Cloud destinations (optional)

```bash
pip install spfetch[s3]     # Amazon S3 (s3fs)
pip install spfetch[gcs]    # Google Cloud Storage (gcsfs)
pip install spfetch[azure]  # Azure Data Lake (adlfs)
```

---

### ğŸ“Š Pandas helpers (optional)

```bash
pip install spfetch[pandas]
```

---

# ğŸš€ Quickstart
Since spfetch is built for modern data engineering, all network operations are asynchronous.

---

## ğŸ” 1ï¸âƒ£ Authenticate (Device Code / MFA)

For Local Development (Device Code / MFA)
Works with MFA-enabled accounts. No secrets required.

```python
from spfetch.auth import DeviceCodeAuth
from spfetch.client import SharePointClient

auth = DeviceCodeAuth(
    tenant_id="YOUR_TENANT_ID",
    client_id="YOUR_CLIENT_ID"
)
client = SharePointClient(auth=auth)
```

âœ”ï¸ Works with MFA-enabled accounts  
âœ”ï¸ No secrets required  
âœ”ï¸ Ideal for local development  

For CI/CD & Automated Pipelines (Client Secret)

```python
from spfetch.auth import ClientSecretAuth

auth = ClientSecretAuth(
    tenant_id="YOUR_TENANT_ID",
    client_id="YOUR_CLIENT_ID",
    client_secret="YOUR_CLIENT_SECRET"
)
client = SharePointClient(auth=auth)
```

---

## ğŸ“‚ 2ï¸âƒ£ List a Folder

```python
import asyncio

async def main():
    items = await client.ls(
        hostname="tenant.sharepoint.com",
        site_path="/sites/MySite",
        folder_path="/Shared Documents/Reports"
    )

    for item in items:
        icon = "ğŸ“" if item["is_folder"] else "ğŸ“„"
        print(f"{icon} {item['name']} | Size: {item['size']} | ID: {item['id']}")

asyncio.run(main())
```

Returns structured metadata for files and folders.

---

## â¬‡ï¸ 3ï¸âƒ£ Streaming Download (Recommended for Large Files)
ğŸ”¥ Files are streamed in chunks â€” no full in-memory loading. Perfect for large CSVs, parquet files, and data pipelines.

### ğŸ–¥ï¸ Download to Local Filesystem

```python
async def main():
    await client.download(
        hostname="tenant.sharepoint.com",
        site_path="/sites/MySite",
        file_path="/Shared Documents/Big/base.csv",
        dest_path="stage/base.csv"
    )
```

---

### â˜ï¸ Download Directly to Cloud Storage

```python
from spfetch.destinations import S3Destination
# from spfetch.destinations import AzureDestination, GCSDestination

async def main():
    # Pass your cloud credentials configuration
    s3_dest = S3Destination(key="AWS_KEY", secret="AWS_SECRET")

    await client.download(
        hostname="tenant.sharepoint.com",
        site_path="/sites/MySite",
        file_path="/Shared Documents/Big/base.csv",
        dest_path="s3://my-bucket/stage/base.csv",
        destination=s3_dest
    )
```

ğŸ”¥ Files are streamed in chunks â€” no full in-memory loading.

Perfect for large CSVs, parquet files, exports, and data pipelines.

---

## ğŸ“Š 4ï¸âƒ£ Read Small Files into pandas (Optional)
`read_df` automatically detects `.csv`, `.xlsx`, or `.xls` and loads it into memory without writing to disk.

```python
async def main():
    df = await client.read_df(
        hostname="tenant.sharepoint.com",
        site_path="/sites/MySite",
        file_path="/Shared Documents/Reports/sales.xlsx",
        sheet_name="Base", # Passed down to pandas
        usecols="B:F",
        skiprows=8
    )
    print(df.head())
```

> âš ï¸ For very large files, prefer `download()` and process them using Spark, Dask, or your data engine of choice.

---

## ğŸ›¡ï¸ Resilience (Handling HTTP 429)
`spfetch` includes a built-in resilience layer. If Microsoft Graph throws an `HTTP 429 Too Many Requests` error, the client will automatically:

1. Catch the error.
2. Read the `Retry-After` header provided by Microsoft.
3. Pause execution asynchronously using Exponential Backoff.
4. Retry the request automatically up to `max_retries` (Default: 3 for API calls, 5 for downloads).

Your pipelines will simply wait and recover gracefully.

---

# ğŸ” Microsoft Entra ID (Azure AD) Setup

To use `spfetch`, create an **App Registration** in Microsoft Entra ID.

---

## ğŸ§­ Setup Steps

1. Create an **App Registration**
2. Copy the `tenant_id`
3. Copy the `client_id`
4. Enable **Public client flows** (Device Code flow)
5. Grant required Microsoft Graph permissions
6. (If needed) Request **Admin Consent**

---

## ğŸ”‘ Required Permissions

Minimum permissions depend on your use case.

### ğŸ“– Read-only access

```
Sites.Read.All
```

### âœï¸ Read & Write access

```
Sites.ReadWrite.All
```

Some tenants may require:

- Admin consent  
- Site-specific permission configuration  

---

# ğŸ§± Design Principles

- ğŸ” MFA-first authentication  
- ğŸŒŠ Streaming over buffering  
- â˜ï¸ Cloud-native architecture  
- ğŸ§© Minimal API surface  
- ğŸ” Explicit behavior over magic  

---

# ğŸ—ºï¸ Roadmap

Planned features:

- ğŸ”‘ `connect_client_secret()` for pipelines / CI  
- ğŸ”„ `sync_folder()` with ETag / Last-Modified support  
- ğŸ§­ Path and URL normalization helpers  
- ğŸ“Š Richer metadata filtering  
- ğŸ” Configurable retry / backoff strategy  

---

# ğŸ¤ Contributing

PRs are welcome!

Please:

- Check our `CONTRIBUTING.md`.
- Ensure all tests pass `via make test`.
- We use the Conventional Commits specification.

---

# ğŸ“„ License

MIT

---

<p align="center">
  Built for modern data workflows ğŸš€
</p>
